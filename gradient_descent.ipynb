{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ2FQ+ZorVt77GxJTOd5ux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyedMisbahGit/FirstGitHubRepo/blob/main/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmAel4A_TAg8",
        "outputId": "cddabea9-3c77-4c58-92b8-f0fcf5aec3ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.8700746372289128\n",
            "Epoch 100, Loss: 1.297503302204833\n",
            "Epoch 200, Loss: 0.9365418909701553\n",
            "Epoch 300, Loss: 1.0338195763756561\n",
            "Epoch 400, Loss: 1.3144985042942265\n",
            "Epoch 500, Loss: 1.7729505368530976\n",
            "Epoch 600, Loss: 1.4062244225642984\n",
            "Epoch 700, Loss: 0.9848462411300345\n",
            "Epoch 800, Loss: 0.5876661542008671\n",
            "Epoch 900, Loss: 1.1719678316053201\n",
            "Epoch 1000, Loss: 1.2861500538198554\n",
            "Epoch 1100, Loss: 1.8912573844758465\n",
            "Epoch 1200, Loss: 0.8278640221498095\n",
            "Epoch 1300, Loss: 1.1563091783514416\n",
            "Epoch 1400, Loss: 0.8809178776769794\n",
            "Epoch 1500, Loss: 1.4404830721522282\n",
            "Epoch 1600, Loss: 1.1996327142001673\n",
            "Epoch 1700, Loss: 1.6950389968890274\n",
            "Epoch 1800, Loss: 1.2525398251988373\n",
            "Epoch 1900, Loss: 1.5205683157335017\n",
            "Epoch 2000, Loss: 0.7103618646873345\n",
            "Epoch 2100, Loss: 1.3452872348447613\n",
            "Epoch 2200, Loss: 1.2316499838047483\n",
            "Epoch 2300, Loss: 0.7515417533518512\n",
            "Epoch 2400, Loss: 1.6065328305031839\n",
            "Epoch 2500, Loss: 1.1950942907862485\n",
            "Epoch 2600, Loss: 0.739791662084082\n",
            "Epoch 2700, Loss: 0.6183423864405867\n",
            "Epoch 2800, Loss: 1.5508619035690496\n",
            "Epoch 2900, Loss: 0.8968756965134868\n",
            "Epoch 3000, Loss: 1.9198455795725773\n",
            "Epoch 3100, Loss: 1.0667898784243088\n",
            "Epoch 3200, Loss: 1.4997298278137254\n",
            "Epoch 3300, Loss: 0.7189130151139602\n",
            "Epoch 3400, Loss: 1.564014618511477\n",
            "Epoch 3500, Loss: 1.1739595906169822\n",
            "Epoch 3600, Loss: 0.6479271425706796\n",
            "Epoch 3700, Loss: 0.8710660894198626\n",
            "Epoch 3800, Loss: 0.9311227214337718\n",
            "Epoch 3900, Loss: 0.8933443015886928\n",
            "Epoch 4000, Loss: 1.2594771020695033\n",
            "Epoch 4100, Loss: 1.8614570817202936\n",
            "Epoch 4200, Loss: 1.1956735159653604\n",
            "Epoch 4300, Loss: 0.8905304063421737\n",
            "Epoch 4400, Loss: 0.54373538958787\n",
            "Epoch 4500, Loss: 1.3216446324119184\n",
            "Epoch 4600, Loss: 1.0961623114674042\n",
            "Epoch 4700, Loss: 0.7815109907964912\n",
            "Epoch 4800, Loss: 0.935962766533498\n",
            "Epoch 4900, Loss: 1.3055842518637792\n",
            "Epoch 5000, Loss: 1.1162009597121194\n",
            "Epoch 5100, Loss: 1.7935599696811126\n",
            "Epoch 5200, Loss: 1.345672593574633\n",
            "Epoch 5300, Loss: 0.9086578427198935\n",
            "Epoch 5400, Loss: 0.9855188875742733\n",
            "Epoch 5500, Loss: 0.811739355446865\n",
            "Epoch 5600, Loss: 1.0694829954883585\n",
            "Epoch 5700, Loss: 0.8576822313333877\n",
            "Epoch 5800, Loss: 1.8964981220031414\n",
            "Epoch 5900, Loss: 0.9367524587066126\n",
            "Epoch 6000, Loss: 0.9954495649619803\n",
            "Epoch 6100, Loss: 1.0448137547766208\n",
            "Epoch 6200, Loss: 0.842291075630154\n",
            "Epoch 6300, Loss: 0.9377830753918589\n",
            "Epoch 6400, Loss: 1.4344108069220027\n",
            "Epoch 6500, Loss: 0.7957716659109585\n",
            "Epoch 6600, Loss: 1.062619369904003\n",
            "Epoch 6700, Loss: 1.627082514009385\n",
            "Epoch 6800, Loss: 1.7097982336311564\n",
            "Epoch 6900, Loss: 1.466920246657579\n",
            "Epoch 7000, Loss: 1.051064618571016\n",
            "Epoch 7100, Loss: 1.1133193938932149\n",
            "Epoch 7200, Loss: 1.3803558066829646\n",
            "Epoch 7300, Loss: 0.7714444931095794\n",
            "Epoch 7400, Loss: 0.9122596634867823\n",
            "Epoch 7500, Loss: 0.8705364176327925\n",
            "Epoch 7600, Loss: 1.4365517130589642\n",
            "Epoch 7700, Loss: 1.389177420664266\n",
            "Epoch 7800, Loss: 1.0445561415830236\n",
            "Epoch 7900, Loss: 0.5953510827070365\n",
            "Epoch 8000, Loss: 1.1057596608696925\n",
            "Epoch 8100, Loss: 1.2897799580144065\n",
            "Epoch 8200, Loss: 0.5970681973830021\n",
            "Epoch 8300, Loss: 0.8591461726862634\n",
            "Epoch 8400, Loss: 1.8473471428494872\n",
            "Epoch 8500, Loss: 1.034962244182012\n",
            "Epoch 8600, Loss: 1.6191008189912743\n",
            "Epoch 8700, Loss: 0.9182748789964928\n",
            "Epoch 8800, Loss: 1.2389287565128388\n",
            "Epoch 8900, Loss: 1.53869855882481\n",
            "Epoch 9000, Loss: 1.9874302182918862\n",
            "Epoch 9100, Loss: 0.6070259279361737\n",
            "Epoch 9200, Loss: 0.8153288799331665\n",
            "Epoch 9300, Loss: 1.887017602087748\n",
            "Epoch 9400, Loss: 1.107985011258008\n",
            "Epoch 9500, Loss: 0.54250027645605\n",
            "Epoch 9600, Loss: 1.7335099025154515\n",
            "Epoch 9700, Loss: 0.8002342192382088\n",
            "Epoch 9800, Loss: 1.8745035245492738\n",
            "Epoch 9900, Loss: 0.6525448845112917\n",
            "\n",
            "Final Weight: -0.6650910680721227\n",
            "Final Bias: -1.2303537923680532\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def sigmoid(X):\n",
        "  return 1/(1 + np.exp(-X))\n",
        "\n",
        "def mse_loss(fx, Y):\n",
        "    return 0.5 * (fx - Y) ** 2\n",
        "\n",
        "def gradient_descent(X,Y,learning_rate = 0.01,epochs = 10000):\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    w,b = np.random.randn(),np.random.randn()\n",
        "    fx = sigmoid(w*X + b)\n",
        "\n",
        "    loss = mse_loss(fx, Y)\n",
        "\n",
        "    gradient_w = (fx - Y)*(fx)*(1 - fx)*X\n",
        "    gradient_b = (fx - Y)*(fx)*(1 - fx)\n",
        "\n",
        "    w -= learning_rate*gradient_w\n",
        "    b -= learning_rate*gradient_b\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "  return w,b\n",
        "\n",
        "X,Y = 1,2\n",
        "w_a,b_a = gradient_descent(X,Y)\n",
        "print(f\"\\nFinal Weight: {w_a}\\nFinal Bias: {b_a}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqByLrbEnp4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}